{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franckbizimana/Wamungu/blob/main/NLP_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ea7d857",
      "metadata": {
        "id": "9ea7d857"
      },
      "source": [
        "# Sentiment Analysis using Natural Language Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30eb4748",
      "metadata": {
        "id": "30eb4748"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3861e4f2",
      "metadata": {
        "id": "3861e4f2"
      },
      "source": [
        "## Table of contents:\n",
        "\n",
        "### 1. Data Acquisition\n",
        "### 2. Data pre-processing and data cleaning\n",
        "### 3. Sentiment Analysis\n",
        "### 4. Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1719b494",
      "metadata": {
        "id": "1719b494"
      },
      "source": [
        "In this notebook I am demonstrating how to pre-process and clean text data for Natural Lanuage Processing applications and how to perform Sentiment Analysis and Text Classification on the data.\n",
        "\n",
        "Pre-processing and data cleaning are crucial steps in any NLP (Natural Language Processing) project. These steps help to ensure that the data used in the project is of high quality, and ready to be used for further analysis and modeling. The following are the common pre-processing and data cleaning steps in an NLP project:\n",
        "\n",
        "1. **Data collection:** The first step is to collect the data that will be used in the project. This can be done from various sources such as text files, web pages, databases, etc.<br><br>\n",
        "\n",
        "2. **Data cleaning:** Remove any irrelevant or redundant information from the data such as special characters, punctuation marks, numbers, etc. This step also involves correcting any spelling mistakes or typos in the data ('recieve' > 'receive', 'brocoli' > 'broccoli').<br><br>\n",
        "\n",
        "3. **Text normalization:** Convert all the text data into a uniform format. This involves converting all the text to lowercase or uppercase, converting slang words or ackronyms ( e.g. lol, gn), expanding contractions (can't to can not), removing stop words (a, the, and, but), stemming or lemmatizing the words to reduce words to their base or root form.<br><br>\n",
        "\n",
        "  - **Stemming:** Stemming is the process of reducing words to their base form by removing suffixes. e.g. the root word in 'writing' and 'written' is 'write'. You get rid of 'ing', 'ed' ,'en'. Stemming algorithms are fast and efficient, but they can sometimes produce non-words or words with a different meaning than the original word.<br><br>\n",
        "\n",
        "  - **Lemmatization:** Lemmatization, on the other hand, is the process of reducing words to their base form using a morphological analysis of words. For e.g. the word 'better'. If we use a stemming algorithm to reduce this word to its base form, it will likely produce 'bett' as the stem. However, this stem is not a meaningful word and does not accurately represent the original word. Lemmatization on the other hand, will reduce the word to its base form or lemma of the word \"better\" would be \"good\".\n",
        "\n",
        "    The choice between stemming and lemmatization will depend on the specific requirements of   the NLP project and the trade-off between speed and accuracy.<br><br>\n",
        "\n",
        "4. **Tokenization:** Tokenization is the process of splitting the text data into smaller chunks or tokens, such as words, phrases, or sentences. This step helps in preparing the text data for further analysis and modeling.<br><br>\n",
        "\n",
        "5. **Text vectorization:** Text vectorization is the process of transforming data into a numerical format that can be used for analysis and modeling. This involves converting the text data into numerical vectors using techniques such as bag of words, TF-IDF, or word embeddings."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN79B7qRz0Vm",
        "outputId": "5ca2af89-7730-4a00-e7c7-75de25a0a3e4"
      },
      "id": "IN79B7qRz0Vm",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "93aa6823",
      "metadata": {
        "id": "93aa6823",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49db8ccf-462b-4db2-c63f-917e1f1667ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━\u001b[0m \u001b[32m184.3/232.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install PyPDF2\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef0645f5",
      "metadata": {
        "id": "ef0645f5"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5416dd",
      "metadata": {
        "id": "9b5416dd"
      },
      "source": [
        "## Different ways of importing text data\n",
        "\n",
        "1. Web scrapping\n",
        "2. Pdf/Word Files\n",
        "\n",
        "The `requests.get(url)` function is used to fetch the HTML content of the **url** specified. The HTML content is then passed to the **BeautifulSoup constructor**, which returns a **BeautifulSoup object** that can be used to parse the HTML.\n",
        "\n",
        "`soup.find_all(\"p\")[:3]` expression is used to find all the **<p>** elements in the HTML and select only the first three. The resulting list of elements is stored in the **paragraphs** variable which is a list of the 3 paragraphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "eb4788f3",
      "metadata": {
        "id": "eb4788f3",
        "outputId": "bfaa62dc-cbe7-41f0-efcb-8d390a76c31f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Editorial Lead, AI Models Writer Natural language processing (NLP) is a subfield of computer science and artificial intelligence (AI) that uses machine learning to enable computers to understand and communicate with human language.\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.ibm.com/topics/natural-language-processing#:~:text=the%20next%20step-,What%20is%20natural%20language%20processing%3F,same%20way%20human%20beings%20can.\"\n",
        "\n",
        "# this helps you to go to the website to fetch the content\n",
        "response = requests.get(url)\n",
        "\n",
        "# BeautifulSoup constructor takes the text as input and returns a BeautifulSoup object.\n",
        "# BeautifulSoup object makes it easier to parse and extract information from the HTML content.\n",
        "soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "# Find the first paragraph element in the HTML\n",
        "paragraphs = soup.find_all(\"p\")[:3]\n",
        "\n",
        "# Extract the text from the selected paragraphs and concatenate them\n",
        "combined_paragraph = ' '.join([p.text for p in paragraphs])\n",
        "\n",
        "print(combined_paragraph)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b7c235c3",
      "metadata": {
        "id": "b7c235c3"
      },
      "outputs": [],
      "source": [
        "#We will work with a pdf document this time.\n",
        "# Import the library reqiured to read from pdf documents.\n",
        "import PyPDF2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbfed4c3",
      "metadata": {
        "id": "fbfed4c3"
      },
      "source": [
        "Here I am using a sample pdf file that has a small explanation about NLP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ce2e791f",
      "metadata": {
        "id": "ce2e791f",
        "outputId": "b05dfb6c-4b5e-418f-adfd-46b15d335469",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Natural language processing (NLP) is a field of study focused on making it # possible for  \n",
            "computers to read, understand, and generate human language. NLP is an interdisciplinary  \n",
            "field that combines computer science, AI, and linguistics.   \n",
            "  \n",
            "The process of NLP includes several steps, such as tokenization, stop word removal,  \n",
            "stemming and lemmat ization, and more.  {}  \n",
            "  \n",
            "In tokenization, we break down the text into individual words, phrases, symbols, or other  \n",
            "elements.   \n",
            "  \n",
            "Stop word removal involves removing commonly used words such as and, the, a, etc. that  \n",
            "do not contribute much to the meaning of the text.   \n",
            "  \n",
            "Stemming and lemmatization, on the other hand, are techniques to reduce words to their  \n",
            "root form. After the text has been cleaned and pre -processed, it can be used for various \n",
            "NLP tasks such as sentiment analysis, text classification, la nguage translation, and more !  \n",
            "  \n",
            "The effectiveness of these tasks greatly depends on the quality of the pre -processing step,  \n",
            "making it a crucial step in NLP.   \n"
          ]
        }
      ],
      "source": [
        "# Open the PDF file\n",
        "with open(\"NLP.pdf\", \"rb\") as file:\n",
        "    # Create a PDF object\n",
        "    pdf = PyPDF2.PdfReader(file)\n",
        "\n",
        "    # Initialize a variable to store the extracted text\n",
        "    corpus = \"\"\n",
        "\n",
        "    # Extract the text from each page of the PDF. (We have only one page)\n",
        "    for page in pdf.pages:\n",
        "        corpus += page.extract_text()\n",
        "\n",
        "    # Print the extracted text\n",
        "    print(corpus)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ece151bb",
      "metadata": {
        "id": "ece151bb"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adfb0152",
      "metadata": {
        "id": "adfb0152"
      },
      "source": [
        "### Convert all text to lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "64e4103a",
      "metadata": {
        "id": "64e4103a",
        "outputId": "dfbf11a3-4c73-452e-88e9-5269318bae50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natural language processing (nlp) is a field of study focused on making it # possible for  \n",
            "computers to read, understand, and generate human language. nlp is an interdisciplinary  \n",
            "field that combines computer science, ai, and linguistics.   \n",
            "  \n",
            "the process of nlp includes several steps, such as tokenization, stop word removal,  \n",
            "stemming and lemmat ization, and more.  {}  \n",
            "  \n",
            "in tokenization, we break down the text into individual words, phrases, symbols, or other  \n",
            "elements.   \n",
            "  \n",
            "stop word removal involves removing commonly used words such as and, the, a, etc. that  \n",
            "do not contribute much to the meaning of the text.   \n",
            "  \n",
            "stemming and lemmatization, on the other hand, are techniques to reduce words to their  \n",
            "root form. after the text has been cleaned and pre -processed, it can be used for various \n",
            "nlp tasks such as sentiment analysis, text classification, la nguage translation, and more !  \n",
            "  \n",
            "the effectiveness of these tasks greatly depends on the quality of the pre -processing step,  \n",
            "making it a crucial step in nlp.   \n"
          ]
        }
      ],
      "source": [
        "# Convert the text to lowercase\n",
        "corpus = corpus.lower()\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42587bd1",
      "metadata": {
        "id": "42587bd1"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac86c5f2",
      "metadata": {
        "id": "ac86c5f2"
      },
      "source": [
        "### Tokenization\n",
        "To tokenize the text, you can use the `word_tokenize` function from the `nltk` library."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Punkt** tokenizer is a pre-trained model that is trained to detect sentence boundaries in text. It can be used to tokenize a given text into a list of sentences."
      ],
      "metadata": {
        "id": "yy0GeRKTsRUx"
      },
      "id": "yy0GeRKTsRUx"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "0838eb04",
      "metadata": {
        "id": "0838eb04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab401e10-c851-428e-a3ce-b461b07359d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "b10d15ed",
      "metadata": {
        "id": "b10d15ed",
        "outputId": "788de698-6bb9-4e61-8768-ed07d19e81cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', '(', 'nlp', ')', 'is', 'a', 'field', 'of', 'study', 'focused', 'on', 'making', 'it', '#', 'possible', 'for', 'computers', 'to', 'read', ',', 'understand', ',', 'and', 'generate', 'human', 'language', '.', 'nlp', 'is', 'an', 'interdisciplinary', 'field', 'that', 'combines', 'computer', 'science', ',', 'ai', ',', 'and', 'linguistics', '.', 'the', 'process', 'of', 'nlp', 'includes', 'several', 'steps', ',', 'such', 'as', 'tokenization', ',', 'stop', 'word', 'removal', ',', 'stemming', 'and', 'lemmat', 'ization', ',', 'and', 'more', '.', '{', '}', 'in', 'tokenization', ',', 'we', 'break', 'down', 'the', 'text', 'into', 'individual', 'words', ',', 'phrases', ',', 'symbols', ',', 'or', 'other', 'elements', '.', 'stop', 'word', 'removal', 'involves', 'removing', 'commonly', 'used', 'words', 'such', 'as', 'and', ',', 'the', ',', 'a', ',', 'etc', '.', 'that', 'do', 'not', 'contribute', 'much', 'to', 'the', 'meaning', 'of', 'the', 'text', '.', 'stemming', 'and', 'lemmatization', ',', 'on', 'the', 'other', 'hand', ',', 'are', 'techniques', 'to', 'reduce', 'words', 'to', 'their', 'root', 'form', '.', 'after', 'the', 'text', 'has', 'been', 'cleaned', 'and', 'pre', '-processed', ',', 'it', 'can', 'be', 'used', 'for', 'various', 'nlp', 'tasks', 'such', 'as', 'sentiment', 'analysis', ',', 'text', 'classification', ',', 'la', 'nguage', 'translation', ',', 'and', 'more', '!', 'the', 'effectiveness', 'of', 'these', 'tasks', 'greatly', 'depends', 'on', 'the', 'quality', 'of', 'the', 'pre', '-processing', 'step', ',', 'making', 'it', 'a', 'crucial', 'step', 'in', 'nlp', '.']\n"
          ]
        }
      ],
      "source": [
        "# Tokenize the text by words\n",
        "tokens = nltk.word_tokenize(corpus)\n",
        "\n",
        "# Print the tokens\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "HUZVZtwpqiBo"
      },
      "id": "HUZVZtwpqiBo"
    },
    {
      "cell_type": "markdown",
      "id": "6b1234c6",
      "metadata": {
        "id": "6b1234c6"
      },
      "source": [
        "### Removing Punctuations\n",
        "To remove punctuations from the tokenized corpus, you can use the string module in Python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "e-z06MabkVXw",
        "outputId": "c97b5bea-bf51-4365-f9ff-858a703dc59d"
      },
      "id": "e-z06MabkVXw",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e827b03d",
      "metadata": {
        "id": "e827b03d"
      },
      "source": [
        "The code below uses a list comprehension to iterate over the tokens and check if each token is not in the `string.punctuation` list. If a token is not in the string.punctuation list, it is added to a new list called tokens_without_punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "1d966e4d",
      "metadata": {
        "id": "1d966e4d",
        "outputId": "22ffc03f-1c74-43d2-eef5-9717ce87fb75",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'field', 'of', 'study', 'focused', 'on', 'making', 'it', 'possible', 'for', 'computers', 'to', 'read', 'understand', 'and', 'generate', 'human', 'language', 'nlp', 'is', 'an', 'interdisciplinary', 'field', 'that', 'combines', 'computer', 'science', 'ai', 'and', 'linguistics', 'the', 'process', 'of', 'nlp', 'includes', 'several', 'steps', 'such', 'as', 'tokenization', 'stop', 'word', 'removal', 'stemming', 'and', 'lemmat', 'ization', 'and', 'more', 'in', 'tokenization', 'we', 'break', 'down', 'the', 'text', 'into', 'individual', 'words', 'phrases', 'symbols', 'or', 'other', 'elements', 'stop', 'word', 'removal', 'involves', 'removing', 'commonly', 'used', 'words', 'such', 'as', 'and', 'the', 'a', 'etc', 'that', 'do', 'not', 'contribute', 'much', 'to', 'the', 'meaning', 'of', 'the', 'text', 'stemming', 'and', 'lemmatization', 'on', 'the', 'other', 'hand', 'are', 'techniques', 'to', 'reduce', 'words', 'to', 'their', 'root', 'form', 'after', 'the', 'text', 'has', 'been', 'cleaned', 'and', 'pre', '-processed', 'it', 'can', 'be', 'used', 'for', 'various', 'nlp', 'tasks', 'such', 'as', 'sentiment', 'analysis', 'text', 'classification', 'la', 'nguage', 'translation', 'and', 'more', 'the', 'effectiveness', 'of', 'these', 'tasks', 'greatly', 'depends', 'on', 'the', 'quality', 'of', 'the', 'pre', '-processing', 'step', 'making', 'it', 'a', 'crucial', 'step', 'in', 'nlp']\n"
          ]
        }
      ],
      "source": [
        "# Remove punctuations from the tokens\n",
        "tokens_without_punctuation = [token for token in tokens if token not in string.punctuation]\n",
        "\n",
        "# Print the tokens without punctuation\n",
        "print(tokens_without_punctuation)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04d90a18",
      "metadata": {
        "id": "04d90a18"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "425e52f1",
      "metadata": {
        "id": "425e52f1"
      },
      "source": [
        "### Remove stop words\n",
        "To remove stop words from the tokenized corpus, you can use the `stopwords` corpus from the `nltk` library"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get a list of stop words in English\n",
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_Z-G-2Tk922",
        "outputId": "597d6b39-e03f-4c3c-e187-9d8049fde20d"
      },
      "id": "t_Z-G-2Tk922",
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjW5HSpZlM3N",
        "outputId": "57158217-05d1-4cfd-bdcc-1ece1c088204"
      },
      "id": "PjW5HSpZlM3N",
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"needn't\", 'doesn', \"shan't\", 'ours', 'any', \"it's\", 'have', \"we're\", \"mightn't\", 'will', 'hasn', 'on', 'about', 'yourselves', 'shan', 'while', 'here', 'we', 'further', 'll', 'no', 'they', \"i've\", 'was', 'couldn', \"it'll\", 'out', 're', \"doesn't\", \"they're\", 'my', 'these', \"we'd\", 'from', 'being', 'theirs', 'but', \"couldn't\", \"it'd\", 'it', 'under', 'a', 'each', 'most', 'or', 'through', 'against', \"i'll\", 'had', \"hasn't\", 'i', 'has', \"she's\", 'that', \"won't\", 'in', 't', 's', 'm', 'this', 'where', 'himself', 'nor', 'itself', 'between', 'him', 'all', 'because', 'am', 'not', 'above', 'me', 'other', 'why', 'what', \"she'll\", \"we've\", 'to', 'myself', \"wasn't\", 'does', 'hadn', 'off', 'd', \"haven't\", 'same', 'them', 'ma', 'whom', \"wouldn't\", 'yourself', 'were', 'can', 'after', 'until', 'down', \"he'd\", 'when', 'own', 'weren', \"weren't\", \"isn't\", 'again', 'wasn', \"hadn't\", 'into', 'up', 'before', 'for', \"you're\", 'now', 'how', 'their', \"he'll\", \"they'd\", 'once', 'themselves', 'very', \"she'd\", 'below', 'needn', 'those', 'and', 'who', 'don', 'during', 'mustn', 'by', \"they've\", 'be', \"mustn't\", 'won', \"i'd\", 'over', \"they'll\", 'doing', 'is', 'she', \"didn't\", \"i'm\", 'only', 'which', \"aren't\", 'his', 'your', 'with', 'too', 'mightn', 'of', 'the', 'y', 've', 'as', 'such', 'an', 'few', 'haven', 'should', 'than', 'did', 'you', 'then', \"you'd\", 'he', 'just', 'more', 'having', 'at', \"he's\", 'both', 'some', 'its', \"don't\", \"you'll\", 'yours', 'been', \"we'll\", 'aren', 'didn', 'shouldn', 'ourselves', \"you've\", 'our', 'isn', \"should've\", 'there', 'do', 'are', 'hers', \"shouldn't\", 'so', 'ain', 'her', 'wouldn', 'if', 'herself', 'o', \"that'll\"}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37c176b0",
      "metadata": {
        "id": "37c176b0"
      },
      "source": [
        "The code below uses the `stopwords.words` function to get a list of stop words in English and stores the list in a set called stop_words. Then it uses a list comprehension to iterate over the tokens without punctuation and check if each token is not in the stop_words set. If a token is not in the stop_words set, it is added to a new list called tokens_without_stop_words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "b967b10d",
      "metadata": {
        "id": "b967b10d",
        "outputId": "3ec1b286-a62e-4049-9c1a-797b69e29ad2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'nlp', 'field', 'study', 'focused', 'making', 'possible', 'computers', 'read', 'understand', 'generate', 'human', 'language', 'nlp', 'interdisciplinary', 'field', 'combines', 'computer', 'science', 'ai', 'linguistics', 'process', 'nlp', 'includes', 'several', 'steps', 'tokenization', 'stop', 'word', 'removal', 'stemming', 'lemmat', 'ization', 'tokenization', 'break', 'text', 'individual', 'words', 'phrases', 'symbols', 'elements', 'stop', 'word', 'removal', 'involves', 'removing', 'commonly', 'used', 'words', 'etc', 'contribute', 'much', 'meaning', 'text', 'stemming', 'lemmatization', 'hand', 'techniques', 'reduce', 'words', 'root', 'form', 'text', 'cleaned', 'pre', '-processed', 'used', 'various', 'nlp', 'tasks', 'sentiment', 'analysis', 'text', 'classification', 'la', 'nguage', 'translation', 'effectiveness', 'tasks', 'greatly', 'depends', 'quality', 'pre', '-processing', 'step', 'making', 'crucial', 'step', 'nlp']\n"
          ]
        }
      ],
      "source": [
        "# Remove stop words from the tokens\n",
        "tokens_without_stop_words = [token for token in tokens_without_punctuation if token not in stop_words]\n",
        "\n",
        "# Print the tokens without stop words\n",
        "print(tokens_without_stop_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "4096f810",
      "metadata": {
        "id": "4096f810",
        "outputId": "923b057f-d2dd-4ffe-bbfc-b67aa6a6d256",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total words before removing stop words - 160\n",
            "Total words left after stop words removal - 91\n"
          ]
        }
      ],
      "source": [
        "print('Total words before removing stop words -',len(tokens_without_punctuation))\n",
        "print('Total words left after stop words removal -',len(tokens_without_stop_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e4d4ad6",
      "metadata": {
        "id": "7e4d4ad6"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a10e2c8c",
      "metadata": {
        "id": "a10e2c8c"
      },
      "source": [
        "### Stemming"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "a44f268e",
      "metadata": {
        "id": "a44f268e",
        "outputId": "ec8aee34-3bcf-4be1-9a4f-2998c274c2a6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natur', 'languag', 'process', 'nlp', 'field', 'studi', 'focus', 'make', 'possibl', 'comput', 'read', 'understand', 'gener', 'human', 'languag', 'nlp', 'interdisciplinari', 'field', 'combin', 'comput', 'scienc', 'ai', 'linguist', 'process', 'nlp', 'includ', 'sever', 'step', 'token', 'stop', 'word', 'remov', 'stem', 'lemmat', 'izat', 'token', 'break', 'text', 'individu', 'word', 'phrase', 'symbol', 'element', 'stop', 'word', 'remov', 'involv', 'remov', 'commonli', 'use', 'word', 'etc', 'contribut', 'much', 'mean', 'text', 'stem', 'lemmat', 'hand', 'techniqu', 'reduc', 'word', 'root', 'form', 'text', 'clean', 'pre', '-process', 'use', 'variou', 'nlp', 'task', 'sentiment', 'analysi', 'text', 'classif', 'la', 'nguag', 'translat', 'effect', 'task', 'greatli', 'depend', 'qualiti', 'pre', '-process', 'step', 'make', 'crucial', 'step', 'nlp']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_tokens = [stemmer.stem(token) for token in tokens_without_stop_words]\n",
        "print(stemmed_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ace9e5",
      "metadata": {
        "id": "e7ace9e5"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31609e48",
      "metadata": {
        "id": "31609e48"
      },
      "source": [
        "Observe how stemming creates non english words."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16f40322",
      "metadata": {
        "id": "16f40322"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3ed999b",
      "metadata": {
        "id": "a3ed999b"
      },
      "source": [
        "### Lemmatization\n",
        "The WordNet corpus is a lexical database of English words, developed by Princeton University. It groups words into sets of synonyms and provides short definitions and example sentences for each word.\n",
        "\n",
        "In lemmatization, the wordnet corpus is used to determine the base form of a word. For example, the lemma of \"better\" is \"good\". The lemmatizer uses the context of a word to determine its correct lemma based on its definition in the wordnet corpus. This is more sophisticated than stemming, which simply removes the suffixes from words without considering the meaning of the word.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae00c1ae",
      "metadata": {
        "id": "ae00c1ae"
      },
      "source": [
        "In the code below, the `nltk.download('omw-1.4')` function downloads the Open Multilingual Wordnet (OMW) data package version 1.4 for the Natural Language Toolkit (NLTK) library in Python.\n",
        "\n",
        "The Open Multilingual Wordnet is a database of synonyms and related words in over 300 languages. It provides a hierarchical structure for words and concepts that allows for tasks such as word sense disambiguation and semantic similarity calculation.\n",
        "\n",
        "By downloading the OMW data package, you can use it within your NLTK-based project to perform various natural language processing tasks, such as lemmatization, word sense disambiguation, and word similarity computation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "f3a0c46b",
      "metadata": {
        "id": "f3a0c46b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa5bb034-4c56-4654-fb16-6f2370b52416"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'processing', 'nlp', 'field', 'study', 'focused', 'making', 'possible', 'computer', 'read', 'understand', 'generate', 'human', 'language', 'nlp', 'interdisciplinary', 'field', 'combine', 'computer', 'science', 'ai', 'linguistics', 'process', 'nlp', 'includes', 'several', 'step', 'tokenization', 'stop', 'word', 'removal', 'stemming', 'lemmat', 'ization', 'tokenization', 'break', 'text', 'individual', 'word', 'phrase', 'symbol', 'element', 'stop', 'word', 'removal', 'involves', 'removing', 'commonly', 'used', 'word', 'etc', 'contribute', 'much', 'meaning', 'text', 'stemming', 'lemmatization', 'hand', 'technique', 'reduce', 'word', 'root', 'form', 'text', 'cleaned', 'pre', '-processed', 'used', 'various', 'nlp', 'task', 'sentiment', 'analysis', 'text', 'classification', 'la', 'nguage', 'translation', 'effectiveness', 'task', 'greatly', 'depends', 'quality', 'pre', '-processing', 'step', 'making', 'crucial', 'step', 'nlp']\n"
          ]
        }
      ],
      "source": [
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens_without_stop_words]\n",
        "print(lemmatized_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`WordNetLemmatizer` by default lemmatizes words as nouns. Therefore, it doesn’t convert `\"making\"` to `\"make\"` because \"making\" as a noun (e.g., \"the making of a movie\") is already in its base form.\n",
        "\n",
        "To correctly lemmatize verbs like `\"making\"` to `\"make\"`, you need to specify the part of speech (POS) as a verb ('v')."
      ],
      "metadata": {
        "id": "w1ZhkpVTl4-y"
      },
      "id": "w1ZhkpVTl4-y"
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(token, pos='v') for token in tokens_without_stop_words]\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "id": "J09ZfyQmmGjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63b1e73c-072e-4853-e5db-67e16201abd7"
      },
      "id": "J09ZfyQmmGjS",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'process', 'nlp', 'field', 'study', 'focus', 'make', 'possible', 'computers', 'read', 'understand', 'generate', 'human', 'language', 'nlp', 'interdisciplinary', 'field', 'combine', 'computer', 'science', 'ai', 'linguistics', 'process', 'nlp', 'include', 'several', 'step', 'tokenization', 'stop', 'word', 'removal', 'stem', 'lemmat', 'ization', 'tokenization', 'break', 'text', 'individual', 'word', 'phrase', 'symbols', 'elements', 'stop', 'word', 'removal', 'involve', 'remove', 'commonly', 'use', 'word', 'etc', 'contribute', 'much', 'mean', 'text', 'stem', 'lemmatization', 'hand', 'techniques', 'reduce', 'word', 'root', 'form', 'text', 'clean', 'pre', '-processed', 'use', 'various', 'nlp', 'task', 'sentiment', 'analysis', 'text', 'classification', 'la', 'nguage', 'translation', 'effectiveness', 'task', 'greatly', 'depend', 'quality', 'pre', '-processing', 'step', 'make', 'crucial', 'step', 'nlp']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c01b3a5",
      "metadata": {
        "id": "6c01b3a5"
      },
      "source": [
        "### Handling acronyms and slang words\n",
        "\n",
        "create a dictionary that maps acronyms or slang words to their expanded form and then use this dictionary to replace the words in your tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "55254fe2",
      "metadata": {
        "id": "55254fe2",
        "outputId": "b91eef20-33ff-4398-f73c-45ec9f7e3bba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['natural', 'language', 'process', 'natural language processing', 'field', 'study', 'focus', 'make', 'possible', 'computers', 'read', 'understand', 'generate', 'human', 'language', 'natural language processing', 'interdisciplinary', 'field', 'combine', 'computer', 'science', 'artificial intelligence', 'linguistics', 'process', 'natural language processing', 'include', 'several', 'step', 'tokenization', 'stop', 'word', 'removal', 'stem', 'lemmat', 'ization', 'tokenization', 'break', 'text', 'individual', 'word', 'phrase', 'symbols', 'elements', 'stop', 'word', 'removal', 'involve', 'remove', 'commonly', 'use', 'word', 'etc', 'contribute', 'much', 'mean', 'text', 'stem', 'lemmatization', 'hand', 'techniques', 'reduce', 'word', 'root', 'form', 'text', 'clean', 'pre', '-processed', 'use', 'various', 'natural language processing', 'task', 'sentiment', 'analysis', 'text', 'classification', 'la', 'nguage', 'translation', 'effectiveness', 'task', 'greatly', 'depend', 'quality', 'pre', '-processing', 'step', 'make', 'crucial', 'step', 'natural language processing']\n"
          ]
        }
      ],
      "source": [
        "expanded_terms = {\n",
        "    'nlp': 'natural language processing',\n",
        "    'ai': 'artificial intelligence'\n",
        "}\n",
        "\n",
        "expanded_tokens = [expanded_terms.get(token, token) for token in lemmatized_tokens]\n",
        "\n",
        "print(expanded_tokens)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556c027a",
      "metadata": {
        "id": "556c027a"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bec8070e",
      "metadata": {
        "id": "bec8070e"
      },
      "source": [
        "The expanded_tokens list is created by iterating through lemmatized_tokens and using the `.get()` method to retrieve the expanded form of each token from the expanded_terms dictionary. If the expanded form of a token is not found in the dictionary, the original token is used.\n",
        "\n",
        "1. The expression before the **for** statement is executed for each item in the iterable specified after the **for** statement.\n",
        "2. In this case, the iterable is **lemmatized_tokens**, which is a list of words.\n",
        "3. `.get()` is a method that works on dictionaries in Python. It allows you to retrieve the value associated with a specified key in a dictionary. If the key you are searching for does not exist in the dictionary, the `.get()` method will return a default value that you can specify as a second argument to the method. If you don't specify a default value, the `.get()` method will return None by default. This can be useful when working with dictionaries as it allows you to access values in a safe and predictable manner, without the risk of encountering a KeyError if the key is not present in the dictionary.\n",
        "5. The result of each evaluation is added to a new list, which is assigned to the **expanded_acronyms** variable.\n",
        "6. The end result is a list of words where the acronyms have been expanded to their full form.\n",
        "\n",
        "\n",
        "In other words, the list comprehension is essentially going through each word in **lemmatized_tokens**, checking if it is an acronym, and if so, replacing it with its full form from **acronym_dict**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "934a58cb",
      "metadata": {
        "id": "934a58cb"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46232979",
      "metadata": {
        "id": "46232979"
      },
      "source": [
        "### Fixing Typos\n",
        "To fix typos in our list of tokens, we can use spelling correction tools like the `Spellchecker` module in the `nltk` library or the autocorrect library. These tools work by comparing the words in your list of tokens to a dictionary of correctly spelled words, and making suggestions for corrected spellings based on the closest match.\n",
        "\n",
        "Use both one after the other for better results. The spelling correction is not always 100% accurate, and we may need to manually review the suggestions made by the tool to ensure that they are correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "6c0373c3",
      "metadata": {
        "id": "6c0373c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "39ea08aa-aafd-42fe-d8b6-e201c9bcc7c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autocorrect\n",
            "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/622.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m614.4/622.8 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622364 sha256=88dacde63c7cf7a47f5f3d7f30e24b480f6a0598575c95a0eb2cbb8ac3118188\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/90/99/807a5ad861ce5d22c3c299a11df8cba9f31524f23ae6e645cb\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install autocorrect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "2f3e7c38",
      "metadata": {
        "id": "2f3e7c38",
        "outputId": "afe55558-7983-40b4-9cec-e3bf37579076",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tokens: ['natural', 'language', 'process', 'natural language processing', 'field', 'study', 'focus', 'make', 'possible', 'computers', 'read', 'understand', 'generate', 'human', 'language', 'natural language processing', 'interdisciplinary', 'field', 'combine', 'computer', 'science', 'artificial intelligence', 'linguistics', 'process', 'natural language processing', 'include', 'several', 'step', 'tokenization', 'stop', 'word', 'removal', 'stem', 'lemmat', 'ization', 'tokenization', 'break', 'text', 'individual', 'word', 'phrase', 'symbols', 'elements', 'stop', 'word', 'removal', 'involve', 'remove', 'commonly', 'use', 'word', 'etc', 'contribute', 'much', 'mean', 'text', 'stem', 'lemmatization', 'hand', 'techniques', 'reduce', 'word', 'root', 'form', 'text', 'clean', 'pre', '-processed', 'use', 'various', 'natural language processing', 'task', 'sentiment', 'analysis', 'text', 'classification', 'la', 'nguage', 'translation', 'effectiveness', 'task', 'greatly', 'depend', 'quality', 'pre', '-processing', 'step', 'make', 'crucial', 'step', 'natural language processing']\n",
            "Corrected tokens: ['natural', 'language', 'process', 'natural language processing', 'field', 'study', 'focus', 'make', 'possible', 'computers', 'read', 'understand', 'generate', 'human', 'language', 'natural language processing', 'interdisciplinary', 'field', 'combine', 'computer', 'science', 'artificial intelligence', 'linguistics', 'process', 'natural language processing', 'include', 'several', 'step', 'tokenization', 'stop', 'word', 'removal', 'stem', 'lemma', 'station', 'tokenization', 'break', 'text', 'individual', 'word', 'phrase', 'symbols', 'elements', 'stop', 'word', 'removal', 'involve', 'remove', 'commonly', 'use', 'word', 'etc', 'contribute', 'much', 'mean', 'text', 'stem', 'lemmatization', 'hand', 'techniques', 'reduce', 'word', 'root', 'form', 'text', 'clean', 'pre', 'processed', 'use', 'various', 'natural language processing', 'task', 'sentiment', 'analysis', 'text', 'classification', 'la', 'language', 'translation', 'effectiveness', 'task', 'greatly', 'depend', 'quality', 'pre', 'processing', 'step', 'make', 'crucial', 'step', 'natural language processing']\n"
          ]
        }
      ],
      "source": [
        "import autocorrect\n",
        "\n",
        "spell = autocorrect.Speller(lang='en')\n",
        "\n",
        "#tokens = ['This', 'is', 'speling', 'misstake']\n",
        "corrected_tokens = [spell.autocorrect_word(token) for token in expanded_tokens]\n",
        "print('Original tokens:', expanded_tokens)\n",
        "print('Corrected tokens:', corrected_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d82b5b35",
      "metadata": {
        "id": "d82b5b35"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b7592c0",
      "metadata": {
        "id": "2b7592c0"
      },
      "source": [
        "# Sentiment Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdcc6305",
      "metadata": {
        "id": "cdcc6305"
      },
      "source": [
        "To perform sentiment analysis on the list of corrected tokens, we can use various libraries in Python such as:\n",
        "\n",
        "1. **nltk:** The Natural Language Toolkit (nltk) provides a SentimentIntensityAnalyzer class that can be used to calculate the sentiment of a piece of text. We can use the `polarity_scores()` method to obtain the sentiment scores.<br><br>\n",
        "\n",
        "2. **TextBlob:** TextBlob is a Python library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as sentiment analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using TextBlob"
      ],
      "metadata": {
        "id": "AHPMvsl3uNJ9"
      },
      "id": "AHPMvsl3uNJ9"
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "cdf84331",
      "metadata": {
        "id": "cdf84331",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "74b86e27-d916-45ab-bcda-e8e4154e658b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: textblob in /usr/local/lib/python3.11/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.11/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89db96d3",
      "metadata": {
        "id": "89db96d3"
      },
      "source": [
        "This would give you a sentiment polarity score, where;\n",
        "\n",
        "* score closer to 1 indicates a positive sentiment,\n",
        "* score closer to -1 indicates a negative sentiment, and\n",
        "* score closer to 0 indicates a neutral sentiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "id": "8a94a035",
      "metadata": {
        "id": "8a94a035",
        "outputId": "6c91c935-25cf-4f53-b2d6-e2ad3a1e3ec5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "natural language process natural language processing field study focus make possible computers read understand generate human language natural language processing interdisciplinary field combine computer science artificial intelligence linguistics process natural language processing include several step tokenization stop word removal stem lemma station tokenization break text individual word phrase symbols elements stop word removal involve remove commonly use word etc contribute much mean text stem lemmatization hand techniques reduce word root form text clean pre processed use various natural language processing task sentiment analysis text classification la language translation effectiveness task greatly depend quality pre processing step make crucial step natural language processing\n"
          ]
        }
      ],
      "source": [
        "# we need to now join the tokens back to make it as a sentence.\n",
        "\n",
        "from textblob import TextBlob\n",
        "\n",
        "text = \" \".join(corrected_tokens)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the polarity score\n",
        "analysis = TextBlob(text)\n",
        "\n",
        "print(analysis.sentiment)"
      ],
      "metadata": {
        "id": "AiJvToXAm_58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc59a658-8bf2-4300-b30d-e3b7e44006d5"
      },
      "id": "AiJvToXAm_58",
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment(polarity=0.032598039215686284, subjectivity=0.5316176470588236)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bcce773",
      "metadata": {
        "id": "1bcce773"
      },
      "source": [
        "**Polarity**: This is a float value that represents the sentiment polarity of the text. It ranges from -1 to 1, where -1 indicates a highly negative sentiment, 0 indicates a neutral sentiment, and 1 indicates a highly positive sentiment.\n",
        "\n",
        "**subjectivity**: This is a float value that represents the degree of subjectivity or opinion present in the text. It ranges from 0 to 1, where 0 indicates a very objective text with no opinion or sentiment, and 1 indicates a highly subjective text with strong opinions or emotions expressed.\n",
        "\n",
        "In other words, the text is not purely subjective and doesn't express personal opinions, feelings, emotions or subjective interpretations, but it has some subjectivity and is not entirely neutral."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06be224d",
      "metadata": {
        "id": "06be224d"
      },
      "source": [
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaaedfa8",
      "metadata": {
        "id": "aaaedfa8"
      },
      "source": [
        "### Using NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3652416d",
      "metadata": {
        "id": "3652416d"
      },
      "source": [
        "The \"VADER (Valence Aware Dictionary and sEntiment Reasoner)\" lexicon, is a pre-trained lexicon and rule-based sentiment analysis tool. It is part of the Natural Language Toolkit (nltk) library in Python, and is used to determine the sentiment of text data by analyzing the words used in the text and the context in which they are used.\n",
        "\n",
        "The VADER lexicon contains a list of words and their associated sentiment scores, which are based on the words' connotations, intensities, and tendencies to appear in positive, neutral, or negative contexts. When analyzing text, the sentiment analysis tool looks at each word in the text and uses the sentiment scores in the VADER lexicon to determine the overall sentiment of the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "a750d309",
      "metadata": {
        "id": "a750d309",
        "outputId": "6d5194cf-fb78-4cb4-b41f-092f2729dae8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: neutral\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "\n",
        "# Create a SentimentIntensityAnalyzer object\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Calculate the sentiment score for each token\n",
        "scores = [sia.polarity_scores(token) for token in corrected_tokens]\n",
        "\n",
        "# Calculate the average sentiment score for the text\n",
        "sentiment_score = sum(score['compound'] for score in scores) / len(scores)\n",
        "\n",
        "# Check the sentiment of the text\n",
        "if sentiment_score >= 0.05:\n",
        "    sentiment = \"positive\"\n",
        "elif sentiment_score <= -0.05:\n",
        "    sentiment = \"negative\"\n",
        "else:\n",
        "    sentiment = \"neutral\"\n",
        "\n",
        "# Print the sentiment of the text\n",
        "print(\"Sentiment:\", sentiment)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8260c813",
      "metadata": {
        "id": "8260c813"
      },
      "source": [
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64fd570e",
      "metadata": {
        "id": "64fd570e"
      },
      "source": [
        "#### Both the libraries are generating a neutral sentiment output."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1b39b55",
      "metadata": {
        "id": "e1b39b55"
      },
      "source": [
        "    "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}